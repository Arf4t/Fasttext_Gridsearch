{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import itertools\n",
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myTokenizer(text):\n",
    "    return nltk.regexp_tokenize(text, \"\\\\b[a-zA-Z]{3,}\\\\b\")\n",
    "def tokenizeContents(contents):\n",
    "    regex = r'\\b\\w+\\b'\n",
    "    li=re.findall(regex,contents)\n",
    "    return [myTokenizer(content) for content in li]\n",
    "def preprocess():\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('/home/student1/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "    twenty_all = fetch_20newsgroups(subset='all', shuffle=True,remove=('headers', 'footers', 'quotes'))\n",
    "    full1=[\" \".join(data.split(\"\\n\")) for data in twenty_all.data]\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    full= []\n",
    "    tmp=[] \n",
    "    for d in range(len(full1)):\n",
    "        words = word_tokenize(full1[d])\n",
    "        for w in words:\n",
    "            if w not in stopWords:\n",
    "                tmp.append(w)\n",
    "        x=\" \".join(tmp)\n",
    "        tmp=myTokenizer(x)\n",
    "        x=\" \".join(tmp)\n",
    "        tmp=[]\n",
    "        full.append(x)\n",
    "    y=twenty_all.target\n",
    "    X,y=readydocs(full,y,model)\n",
    "    return X,y\n",
    "def readydocs(full,y,model):\n",
    "    add=np.zeros(300,dtype='float32')\n",
    "    doc=np.zeros((18846,300))\n",
    "    l=[]#for empty documents\n",
    "    count=0 #total times the key error is raised\n",
    "    c_names=[] #contains words that are in documents but they don't have vector representations\n",
    "    count_word_exists=0 #checks number of words having vector representations in the current document\n",
    "    del_index=[] # has index of the documnets where no word has a vector representation\n",
    "\n",
    "    rate=[] #hit rate for words having vector representations in a document\n",
    "    for q1 in range(len(full)):\n",
    "        token=tokenizeContents(full[q1])\n",
    "        token = list(filter(None, token))\n",
    "        if len(token)==0:\n",
    "            l.append(q1)\n",
    "            rate.append(0)\n",
    "            doc[q1]=np.zeros(300,dtype='float32')\n",
    "            continue\n",
    "        for q in range(len(token)):\n",
    "            try:\n",
    "                add=add+model.word_vec(\"\".join(token[q]))\n",
    "                count_word_exists=count_word_exists+1\n",
    "\n",
    "            except KeyError:\n",
    "                count+=1\n",
    "                if \"\".join(token[q]) not in c_names:\n",
    "                    c_names.append(\"\".join(token[q]))\n",
    "                continue\n",
    "    #print(add,len(token))\n",
    "        if count_word_exists==0:\n",
    "            del_index.append(q1)\n",
    "            add=np.zeros(300,dtype='float32')\n",
    "            doc[q1]=add\n",
    "            rate.append(0)\n",
    "            continue\n",
    "        rate.append(count_word_exists/len(token))\n",
    "        add=add/count_word_exists\n",
    "        doc[q1]=add\n",
    "        add=np.zeros(300,dtype='float32')\n",
    "        count_word_exists=0\n",
    "    doc=[ value for (i, value) in enumerate(doc) if i not in set(l+del_index) ]\n",
    "    doc=np.asarray(doc)\n",
    "    X,y=delundocs(full,y,l,del_index)###delete unnecessary docs\n",
    "    return X,y\n",
    "def delundocs(full,y,l,del_index):\n",
    "    #y=twenty_all.target                                 ##############deleting docs for all methods\n",
    "    full=[ value for (i, value) in enumerate(full) if i not in set(l+del_index) ]\n",
    "    full_f=full\n",
    "    full=np.asarray(full)\n",
    "    for i in sorted(set(l+del_index),reverse=True):\n",
    "        y=np.delete(y,i)\n",
    "    return full_f,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ftext(k,X,y,train_index,test_index,dim=100,epoch=50,ws=5,lr=0.1):\n",
    "    ##################              Fast text\n",
    "    cmd1=\"fasttext supervised \" ###############################################start\n",
    "    cmd1=cmd1+' -dim '+str(dim)+' -epoch '+str(epoch)+' -ws '+str(ws)+' -lr '+str(lr)+\" -input train20.txt -output model20 >> ftrainresult.txt\"\n",
    "    #print (cmd1)\n",
    "    #return\n",
    "    cmd2=\"fasttext test model20.bin test20.txt >> ftestresult.txt\"\n",
    "    cmd3=\"fasttext predict model20.bin test20.txt > result20.txt\"\n",
    "    c=0\n",
    "    outF = open(\"train20.txt\", \"w\")\n",
    "    out1 = open(\"test20.txt\", \"w\")\n",
    "    for i in range(len(train_index)):\n",
    "        outF.write(\"__label__\")\n",
    "        outF.write(str(y[train_index[i]]))\n",
    "        outF.write(\" \")\n",
    "        try :\n",
    "            outF.write(X[train_index[i]])\n",
    "            outF.write(\"\\n\")\n",
    "        except UnicodeEncodeError:\n",
    "            print(\"i\",i)\n",
    "            outF.write(\"\\n\")\n",
    "            continue\n",
    "    os.system(cmd1)\n",
    "    #print(len(test_index))\n",
    "    c=0\n",
    "    for j in range(len(test_index)):\n",
    "        out1.write(\"__label__\")\n",
    "        out1.write(str(y[test_index[j]]))\n",
    "        out1.write(\" \")\n",
    "        c=c+1\n",
    "        try :\n",
    "            out1.write(X[test_index[j]])\n",
    "            out1.write(\"\\n\")\n",
    "        except UnicodeEncodeError:\n",
    "            print(\"j\",j)\n",
    "            out1.write(\"\\n\")\n",
    "            continue\n",
    "    outF.close()\n",
    "    out1.close()\n",
    "    os.system(cmd2)\n",
    "    os.system(cmd3)\n",
    "    f = open('result20.txt', 'r')\n",
    "    lines = np.asarray([np.int32(\"\".join(re.findall('\\d+', line))) for line in f.readlines()])\n",
    "    f.close()\n",
    "    #self.trupresav(\"fasttext\"+str(k),lines,test_index)\n",
    "    #self.report=classification_report(self.y[test_index],lines,target_names=self.twenty_all.target_names)\n",
    "    #self.reporttocsv('fasttext',i=k)\n",
    "    #print(\"Accuracy score :\",round(accuracy_score(y[test_index],lines),3))\n",
    "    return round(accuracy_score(y[test_index],lines),3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_val=[10,25,50,100,200,250,300]\n",
    "epoch_val=[5,20,25,50,100]\n",
    "ws_val=[5,10]\n",
    "lr_val=[0.05,0.025,0.075,0.1,0.150,0.175]\n",
    "p_grid=dict(dim=dim_val,epoch=epoch_val,ws=ws_val,lr=lr_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CP=[dict(zip(p_grid, x)) for x in itertools.product(*p_grid.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3,shuffle=True,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (420 of 420) |#######################| Elapsed Time: 3:37:31 Time: 3:37:31\n"
     ]
    }
   ],
   "source": [
    "pbar=ProgressBar()\n",
    "ac_list=[]\n",
    "for x in pbar(CP):\n",
    "    for k,(train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        ac_list.append(ftext(k,X,y,train_index,test_index,x['dim'],x['epoch'],x['ws'],x['lr']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st\n",
    "ac_conlis=[]\n",
    "for i in range(0,len(ac_list),3):\n",
    "    ac_conlis.append(st.mean([ac_list[i],ac_list[i+1],ac_list[i+2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=np.argsort(ac_conlis)[::-1][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'dim': 50, 'epoch': 100, 'ws': 5, 'lr': 0.075} -------------------->>>>score : 0.705\n",
      "parameters: {'dim': 50, 'epoch': 100, 'ws': 10, 'lr': 0.05} -------------------->>>>score : 0.704\n",
      "parameters: {'dim': 100, 'epoch': 100, 'ws': 10, 'lr': 0.1} -------------------->>>>score : 0.704\n",
      "parameters: {'dim': 50, 'epoch': 100, 'ws': 10, 'lr': 0.075} -------------------->>>>score : 0.704\n",
      "parameters: {'dim': 50, 'epoch': 100, 'ws': 5, 'lr': 0.05} -------------------->>>>score : 0.704\n",
      "parameters: {'dim': 50, 'epoch': 100, 'ws': 10, 'lr': 0.1} -------------------->>>>score : 0.704\n",
      "parameters: {'dim': 50, 'epoch': 50, 'ws': 5, 'lr': 0.1} -------------------->>>>score : 0.704\n",
      "parameters: {'dim': 50, 'epoch': 50, 'ws': 5, 'lr': 0.175} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 50, 'epoch': 50, 'ws': 5, 'lr': 0.15} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 100, 'epoch': 50, 'ws': 5, 'lr': 0.15} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 25, 'epoch': 100, 'ws': 5, 'lr': 0.1} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 50, 'epoch': 100, 'ws': 5, 'lr': 0.1} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 50, 'epoch': 50, 'ws': 10, 'lr': 0.15} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 50, 'epoch': 25, 'ws': 5, 'lr': 0.175} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 100, 'epoch': 100, 'ws': 10, 'lr': 0.075} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 100, 'epoch': 100, 'ws': 5, 'lr': 0.1} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 100, 'epoch': 100, 'ws': 10, 'lr': 0.15} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 100, 'epoch': 50, 'ws': 10, 'lr': 0.15} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 100, 'epoch': 100, 'ws': 10, 'lr': 0.05} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 50, 'epoch': 50, 'ws': 10, 'lr': 0.1} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 250, 'epoch': 100, 'ws': 10, 'lr': 0.1} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 200, 'epoch': 100, 'ws': 10, 'lr': 0.1} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 25, 'epoch': 100, 'ws': 10, 'lr': 0.075} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 50, 'epoch': 25, 'ws': 10, 'lr': 0.175} -------------------->>>>score : 0.703\n",
      "parameters: {'dim': 100, 'epoch': 100, 'ws': 5, 'lr': 0.075} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 200, 'epoch': 100, 'ws': 10, 'lr': 0.05} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 25, 'epoch': 50, 'ws': 10, 'lr': 0.175} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 250, 'epoch': 100, 'ws': 10, 'lr': 0.175} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 25, 'epoch': 100, 'ws': 5, 'lr': 0.075} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 200, 'epoch': 100, 'ws': 5, 'lr': 0.1} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 100, 'epoch': 100, 'ws': 5, 'lr': 0.05} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 25, 'epoch': 50, 'ws': 10, 'lr': 0.15} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 250, 'epoch': 50, 'ws': 5, 'lr': 0.15} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 200, 'epoch': 100, 'ws': 5, 'lr': 0.075} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 50, 'epoch': 100, 'ws': 10, 'lr': 0.15} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 300, 'epoch': 100, 'ws': 5, 'lr': 0.15} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 250, 'epoch': 100, 'ws': 5, 'lr': 0.1} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 50, 'epoch': 50, 'ws': 10, 'lr': 0.175} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 25, 'epoch': 100, 'ws': 10, 'lr': 0.1} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 200, 'epoch': 50, 'ws': 10, 'lr': 0.15} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 200, 'epoch': 100, 'ws': 10, 'lr': 0.075} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 100, 'epoch': 100, 'ws': 10, 'lr': 0.175} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 200, 'epoch': 100, 'ws': 10, 'lr': 0.15} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 250, 'epoch': 50, 'ws': 10, 'lr': 0.175} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 200, 'epoch': 100, 'ws': 5, 'lr': 0.15} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 250, 'epoch': 100, 'ws': 10, 'lr': 0.15} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 300, 'epoch': 100, 'ws': 10, 'lr': 0.175} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 250, 'epoch': 50, 'ws': 10, 'lr': 0.1} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 50, 'epoch': 50, 'ws': 10, 'lr': 0.075} -------------------->>>>score : 0.702\n",
      "parameters: {'dim': 100, 'epoch': 100, 'ws': 5, 'lr': 0.15} -------------------->>>>score : 0.702\n"
     ]
    }
   ],
   "source": [
    "CP=np.asarray(CP)\n",
    "ac_conlis=np.asarray(ac_conlis)\n",
    "for v in index:\n",
    "    print('parameters:',CP[v],'-------------------->>>>score :',round(ac_conlis[v],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
